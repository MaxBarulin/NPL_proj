# Система анализа текстовых предложений на дубликаты и уникальность

## Описание проекта

Данное приложение предназначено для анализа текстовых предложений с целью выявления дубликатов и уникальных формулировок. Основное назначение — сравнение новых предложений с уже существующей базой и между собой для исключения повторов и выявления действительно новых идей.

Система особенно полезна для:
- Обработки предложений по улучшению процессов в организации
- Анализа обратной связи от пользователей
- Фильтрации дублирующихся идей в системах сбора предложений
- Выявления уникальных формулировок среди большого количества текстов

Приложение работает полностью автономно после первоначальной настройки и не требует постоянного подключения к интернету.

## Требования к системе

- Операционная система: Windows, Linux или macOS
- Свободное место на диске: не менее 2 ГБ (для хранения модели и кэшей)
- Оперативная память: минимум 4 ГБ (рекомендуется 8 ГБ и более для обработки больших объемов данных)
- Python версии 3.8 или выше

## Установка и настройка

### Шаг 1: Подготовка окружения

1. Установите Python с официального сайта https://www.python.org/downloads/ (убедитесь, что при установке отмечен пункт "Add Python to PATH")
2. Откройте командную строку (Windows) или терминал (Linux/macOS)
3. Проверьте установку Python командой:
   ```
   python --version
   ```
   или
   ```
   python3 --version
   ```

### Шаг 2: Установка необходимых пакетов

Выполните следующие команды в командной строке:

```
pip install sentence-transformers
pip install scikit-learn
pip install numpy
```

### Шаг 3: Подготовка данных

1. Подготовьте два списка предложений в формате Python:
   - Список новых предложений (которые нужно проверить)
   - Список существующих предложений (база для сравнения)
2. Откройте файл с программой (например, `analyzer.py`) в текстовом редакторе
3. Найдите раздел с демонстрационными данными:
   ```python
   # 100 НОВЫХ предложений (A) - для демонстрации берем меньше
   new_sentences_demo = [
       # ваши предложения здесь
   ]
   
   # 5000 ПРЕДЛОЖЕНИЙ БАЗЫ (B) - для демонстрации берем меньше
   base_sentences_demo = [
       # ваши предложения здесь
   ]
   ```
4. Замените демонстрационные данные вашими реальными списками:
   - Вместо `new_sentences_demo` вставьте ваш список новых предложений
   - Вместо `base_sentences_demo` вставьте вашу базу существующих предложений
5. Сохраните файл

## Первый запуск (требуется интернет)

При первом запуске программе потребуется доступ в интернет для загрузки языковой модели.

1. Откройте командную строку в папке с программой
2. Выполните команду:
   ```
   python analyzer.py
   ```
3. Дождитесь завершения загрузки модели (это может занять 5-15 минут в зависимости от скорости интернета)
4. Модель будет сохранена в локальную папку для последующей офлайн-работы

## Последующие запуски (без интернета)

После первого запуска программа может работать полностью автономно:

1. Откройте командную строку в папке с программой
2. Выполните команду:
   ```
   python analyzer.py
   ```
3. Программа автоматически загрузит сохраненную модель и выполнит анализ

## Интерпретация результатов

После выполнения анализа программа выводит подробный отчет в консоль и сохраняет результаты в файл `analysis_results.json`.

### Структура отчета:

1. **Кластеры дубликатов** — группы предложений, которые являются дубликатами или очень похожи друг на друга:
   - Для каждого кластера показаны новые предложения (помечены как "НОВЫЕ")
   - Показаны соответствующие предложения из базы (помечены как "СУЩЕСТВУЮЩИЕ")
   - Каждое предложение имеет индекс для идентификации (A для новых, B для базовых)

2. **Уникальные предложения** — предложения, которые не имеют близких аналогов ни в новых данных, ни в базе:
   - Для каждого уникального предложения указаны метрики сходства с базой и другими новыми предложениями
   - Чем ниже значение сходства (ближе к 0), тем более уникально предложение

### Рекомендации по использованию результатов:

- **Предложения в кластерах дубликатов** можно объединить или выбрать наиболее полно сформулированный вариант
- **Уникальные предложения** заслуживают отдельного внимания, так как представляют новые идеи
- **Предложения с низким, но ненулевым сходством** могут быть улучшенными версиями существующих идей

## Файлы кэширования

Для ускорения работы программа создает файлы кэша:

- `user-bge-m3-local/` — папка с языковой моделью (создается при первом запуске)
- `all_embeddings_cache.pkl` — кэш векторных представлений всех предложений
- `analysis_results.json` — результаты последнего анализа в формате JSON

### Важно:
- Если вы изменяете списки предложений, кэш будет автоматически обновлен
- Для полной очистки кэшей (например, при смене модели) удалите файлы кэша и папку модели
- Файл `analysis_results.json` можно открывать в любом текстовом редакторе или специализированных программах для просмотра JSON

## Технические параметры

- **Используемая модель**: `deepvk/USER-bge-m3` — специально обученная модель для обработки русского языка
- **Порог сходства для дубликатов**: 0.85 (значение от 0 до 1, где 1 — полное совпадение)
- **Минимальный размер кластера**: 2 предложения (меньшие группы считаются уникальными)

Эти параметры можно изменить в начале файла программы при необходимости:
```python
THRESHOLD_SIMILARITY = 0.85  # Порог для определения дубликатов
MIN_CLUSTER_SIZE = 2         # Минимальное количество предложений в кластере
```

## Решение проблем

### Если программа выдает ошибку при первом запуске:
- Убедитесь, что у вас есть доступ в интернет
- Проверьте, достаточно ли свободного места на диске (требуется минимум 2 ГБ)
- Убедитесь, что все пакеты установлены корректно (`sentence-transformers`, `scikit-learn`, `numpy`)

### Если результаты кажутся неточными:
- Попробуйте скорректировать порог сходства (`THRESHOLD_SIMILARITY`)
- Убедитесь, что предложения сформулированы четко и без опечаток
- Для очень больших объемов данных (тысячи предложений) может потребоваться больше оперативной памяти

### Если программа работает слишком медленно:
- Убедитесь, что кэши работают (при повторных запусках с теми же данными анализ должен быть быстрее)
- Для очень больших баз данных рекомендуется разбить анализ на части
- Проверьте использование оперативной памяти — при нехватке памяти производительность снижается

## Дополнительная информация

Для получения технической поддержки или улучшения функционала обратитесь к разработчику системы. Все результаты анализа сохраняются в файл `analysis_results.json` и могут быть использованы для дальнейшей обработки или интеграции с другими системами.

Программа распространяется "как есть" без каких-либо гарантий. Автор не несет ответственности за последствия использования результатов анализа в бизнес-процессах без дополнительной проверки.