# Система анализа текстовых предложений на дубликаты и уникальность

## Описание проекта

Данное приложение предназначено для анализа текстовых предложений с целью выявления дубликатов и уникальных формулировок. Основное назначение — сравнение новых предложений с уже существующей базой и между собой для исключения повторов и выявления действительно новых идей.

Система особенно полезна для:
- Обработки предложений по улучшению процессов в организации
- Анализа обратной связи от пользователей
- Фильтрации дублирующихся идей в системах сбора предложений
- Выявления уникальных формулировок среди большого количества текстов

Приложение работает полностью автономно после первоначальной настройки и не требует постоянного подключения к интернету.

## Как это работает: концепция алгоритма

### Общая схема работы

Система использует современные методы обработки естественного языка (NLP) для анализа семантического сходства текстов. Процесс можно представить в виде следующих этапов:

```
[Текстовые предложения] 
        ↓
[Преобразование в числовые векторы (эмбеддинги)]
        ↓
[Анализ сходства между векторами]
        ↓
[Группировка похожих предложений в кластеры]
        ↓
[Классификация: дубликаты или уникальные]
```

### 1. Преобразование текста в числовые векторы (эмбеддинги)

Ключевая концепция алгоритма — преобразование текстовых предложений в числовые векторы высокой размерности. Каждое предложение представляется как точка в многомерном пространстве, где близость точек отражает семантическую близость предложений.

**Как это работает:**
- Специальная нейронная сеть (модель `deepvk/USER-bge-m3`) анализирует текст каждого предложения
- Модель выделяет ключевые смысловые характеристики и преобразует их в числовой вектор (эмбеддинг) размерностью 1024
- Чем ближе два вектора в этом пространстве, тем более похожи по смыслу соответствующие предложения
- Например, предложения "Ускорить согласование документов" и "Сделать процесс утверждения заявок быстрее" будут иметь близкие векторы

**Почему именно эта модель для русского языка:**
- Модель `deepvk/USER-bge-m3` специально обучена на русскоязычных данных
- Она учитывает особенности русского языка: падежи, склонения, синонимы, контекстные зависимости
- На международных бенчмарках (ruMTEB) эта модель показывает лучшие результаты для русского языка среди моделей подобного размера
- Модель понимает не только прямые совпадения слов, но и смысловую близость даже при разной формулировке

### 2. Измерение сходства между предложениями

Для определения сходства между предложениями используется косинусное расстояние между их векторными представлениями.

**Косинусное сходство:**
- Значение от 0 до 1, где 1 означает полное совпадение, 0 — полную несхожесть
- Формула: cos(θ) = (A·B) / (||A|| × ||B||), где A и B — векторы предложений
- Преимущество косинусного сходства: оно устойчиво к различиям в длине предложений и фокусируется на смысловом направлении векторов

**Пороги сходства:**
- Порог 0.85 для дубликатов: предложения с сходством выше этого значения считаются дубликатами
- Порог 0.60 для уникальных: предложения с сходством ниже этого значения считаются уникальными
- Эти пороги выбраны эмпирически на основе анализа качества работы с русскими текстами

### 3. Кластеризация предложений

Вместо простого парного сравнения система использует алгоритм кластеризации DBSCAN (Density-Based Spatial Clustering of Applications with Noise) для группировки похожих предложений.

**Как работает кластеризация:**
- Алгоритм автоматически определяет плотные области в векторном пространстве
- Предложения, находящиеся близко друг к другу (выше порога сходства), объединяются в кластеры
- Предложения, не имеющие достаточно близких соседей, считаются уникальными (шумом)
- Преимущество перед парным сравнением: кластеризация находит все взаимосвязанные дубликаты, а не только пары

**Пример работы кластеризации:**
```
Предложения: 
A1: "Ускорить согласование документов"
A2: "Сделать процесс утверждения заявок быстрее"  
B1: "Сократить время ожидания при рассмотрении"

Если A1 и A2 имеют сходство 0.87, A1 и B1 - 0.86, A2 и B1 - 0.84,
то все три предложения будут объединены в один кластер,
даже если между некоторыми парами сходство немного ниже порога
```

### 4. Классификация результатов

Система разделяет все предложения на три категории:

**1. Дубликаты внутри новых предложений:**
- Несколько новых предложений имеют высокое сходство друг с другом
- Пример: "Добавить кнопку настроек" и "Внедрить иконку для быстрого доступа к настройкам"
- Рекомендация: объединить или выбрать наиболее полную формулировку

**2. Дубликаты с существующей базой:**
- Новое предложение имеет высокое сходство с уже существующим в базе
- Пример: "Установить двухфакторную аутентификацию" (новое) и "Внедрить двухэтапную верификацию" (база)
- Рекомендация: рассмотреть как улучшение существующего предложения или отклонить как дубликат

**3. Уникальные предложения:**
- Предложение имеет низкое сходство как с базой, так и с другими новыми предложениями
- Пример: "Провести аудит безопасности IT-инфраструктуры" при отсутствии похожих тем в базе
- Рекомендация: дать высокий приоритет для рассмотрения как новой идеи

### 5. Кэширование для производительности

Для работы с большими объемами данных (тысячи предложений) система использует механизм кэширования:

- **Кэширование модели**: после первого скачивания модель сохраняется локально для офлайн-работы
- **Кэширование эмбеддингов**: один раз рассчитанные векторные представления сохраняются на диск
- **Преимущества**: повторный анализ тех же данных занимает секунды вместо минут, так как не требуется повторное преобразование текста в векторы

## Требования к системе

- Операционная система: Windows, Linux или macOS
- Свободное место на диске: не менее 2 ГБ (для хранения модели и кэшей)
- Оперативная память: минимум 4 ГБ (рекомендуется 8 ГБ и более для обработки больших объемов данных)
- Python версии 3.8 или выше

## Установка и настройка

### Шаг 1: Подготовка окружения

1. Установите Python с официального сайта https://www.python.org/downloads/ (убедитесь, что при установке отмечен пункт "Add Python to PATH")
2. Откройте командную строку (Windows) или терминал (Linux/macOS)
3. Проверьте установку Python командой:
   ```
   python --version
   ```
   или
   ```
   python3 --version
   ```

### Шаг 2: Установка необходимых пакетов

Выполните следующие команды в командной строке:

```
pip install sentence-transformers
pip install scikit-learn
pip install numpy
```

### Шаг 3: Подготовка данных

1. Подготовьте два списка предложений в формате Python:
   - Список новых предложений (которые нужно проверить)
   - Список существующих предложений (база для сравнения)
2. Откройте файл с программой (например, `NPL.py`) в текстовом редакторе
3. Найдите раздел с демонстрационными данными:
   ```python
   # 100 НОВЫХ предложений (A) - для демонстрации берем меньше
   new_sentences_demo = [
       # ваши предложения здесь
   ]
   
   # 5000 ПРЕДЛОЖЕНИЙ БАЗЫ (B) - для демонстрации берем меньше
   base_sentences_demo = [
       # ваши предложения здесь
   ]
   ```
4. Замените демонстрационные данные вашими реальными списками:
   - Вместо `new_sentences_demo` вставьте ваш список новых предложений
   - Вместо `base_sentences_demo` вставьте вашу базу существующих предложений
5. Сохраните файл

## Первый запуск (требуется интернет)

При первом запуске программе потребуется доступ в интернет для загрузки языковой модели.

1. Откройте командную строку в папке с программой
2. Выполните команду:
   ```
   python analyzer.py
   ```
3. Дождитесь завершения загрузки модели (это может занять 5-15 минут в зависимости от скорости интернета)
4. Модель будет сохранена в локальную папку для последующей офлайн-работы

## Последующие запуски (без интернета)

После первого запуска программа может работать полностью автономно:

1. Откройте командную строку в папке с программой
2. Выполните команду:
   ```
   python NPL.py
   ```
3. Программа автоматически загрузит сохраненную модель и выполнит анализ

## Интерпретация результатов

После выполнения анализа программа выводит подробный отчет в консоль и сохраняет результаты в файл `analysis_results.json`.

### Структура отчета:

1. **Кластеры дубликатов** — группы предложений, которые являются дубликатами или очень похожи друг на друга:
   - Для каждого кластера показаны новые предложения (помечены как "НОВЫЕ")
   - Показаны соответствующие предложения из базы (помечены как "СУЩЕСТВУЮЩИЕ")
   - Каждое предложение имеет индекс для идентификации (A для новых, B для базовых)

2. **Уникальные предложения** — предложения, которые не имеют близких аналогов ни в новых данных, ни в базе:
   - Для каждого уникального предложения указаны метрики сходства с базой и другими новыми предложениями
   - Чем ниже значение сходства (ближе к 0), тем более уникально предложение

### Рекомендации по использованию результатов:

- **Предложения в кластерах дубликатов** можно объединить или выбрать наиболее полно сформулированный вариант
- **Уникальные предложения** заслуживают отдельного внимания, так как представляют новые идеи
- **Предложения с низким, но ненулевым сходством** могут быть улучшенными версиями существующих идей

## Файлы кэширования

Для ускорения работы программа создает файлы кэша:

- `user-bge-m3-local/` — папка с языковой моделью (создается при первом запуске)
- `all_embeddings_cache.pkl` — кэш векторных представлений всех предложений
- `analysis_results.json` — результаты последнего анализа в формате JSON

### Важно:
- Если вы изменяете списки предложений, кэш будет автоматически обновлен
- Для полной очистки кэшей (например, при смене модели) удалите файлы кэша и папку модели
- Файл `analysis_results.json` можно открывать в любом текстовом редакторе или специализированных программах для просмотра JSON

## Технические параметры

- **Используемая модель**: `deepvk/USER-bge-m3` — специально обученная модель для обработки русского языка
- **Порог сходства для дубликатов**: 0.85 (значение от 0 до 1, где 1 — полное совпадение)
- **Минимальный размер кластера**: 2 предложения (меньшие группы считаются уникальными)

Эти параметры можно изменить в начале файла программы при необходимости:
```python
THRESHOLD_SIMILARITY = 0.85 # Порог для определения дубликатов
MIN_CLUSTER_SIZE = 2 # Минимальное количество предложений в кластере
```

## Решение проблем

### Если программа выдает ошибку при первом запуске:
- Убедитесь, что у вас есть доступ в интернет
- Проверьте, достаточно ли свободного места на диске (требуется минимум 2 ГБ)
- Убедитесь, что все пакеты установлены корректно (`sentence-transformers`, `scikit-learn`, `numpy`)

### Если результаты кажутся неточными:
- Попробуйте скорректировать порог сходства (`THRESHOLD_SIMILARITY`)
- Убедитесь, что предложения сформулированы четко и без опечаток
- Для очень больших объемов данных (тысячи предложений) может потребоваться больше оперативной памяти

### Если программа работает слишком медленно:
- Убедитесь, что кэши работают (при повторных запусках с теми же данными анализ должен быть быстрее)
- Для очень больших баз данных рекомендуется разбить анализ на части
- Проверьте использование оперативной памяти — при нехватке памяти производительность снижается

## Дополнительная информация

Для получения технической поддержки или улучшения функционала обратитесь к разработчику системы. Все результаты анализа сохраняются в файл `analysis_results.json` и могут быть использованы для дальнейшей обработки или интеграции с другими системами.

Программа распространяется "как есть" без каких-либо гарантий. Автор не несет ответственности за последствия использования результатов анализа в бизнес-процессах без дополнительной проверки.
