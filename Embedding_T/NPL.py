import os
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import pickle
import json
from typing import List, Dict, Tuple, Set
import time
import warnings

# --- –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê ---
MODEL_NAME = 'deepvk/USER-bge-m3'  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
LOCAL_MODEL_PATH = "user-bge-m3-local"  # –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

# –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (—Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º)
BASE_VECTORS_CACHE_FILE = "base_vectors_cache.pkl"
NEW_VECTORS_CACHE_FILE = "new_vectors_cache.pkl"
CLUSTERS_CACHE_FILE = "clusters_cache.json"

# –ü–æ—Ä–æ–≥–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–æ–¥ USER-bge-m3 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
THRESHOLD_SIMILARITY = 0.85  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
MIN_CLUSTER_SIZE = 2  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–ª—è —Å—á–∏—Ç—ã–≤–∞–Ω–∏—è –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç


# --- –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ú–û–î–ï–õ–ò –° –û–§–§–õ–ê–ô–ù-–ü–û–î–î–ï–†–ñ–ö–û–ô ---

def load_model_offline(model_name: str, local_path: str) -> SentenceTransformer:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ—Ñ–ª–∞–π–Ω. –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–æ–ø–∏–∏ –Ω–µ—Ç - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ—Ç, –Ω–æ –Ω–µ –ø—ã—Ç–∞–µ—Ç—Å—è —Å–∫–∞—á–∞—Ç—å.
    """
    if os.path.exists(local_path):
        print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏: '{local_path}'...")
        try:
            model = SentenceTransformer(local_path)
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –æ—Ñ—Ñ–ª–∞–π–Ω-—Ä–∞–±–æ—Ç—ã.")
            return model
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            print("‚ùó –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–∏.")
            raise

    # –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç - –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    print(f"\n{'=' * 60}")
    print(f"‚ùå –õ–û–ö–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ –ù–ï –ù–ê–ô–î–ï–ù–ê: '{local_path}'")
    print(f"‚ùó –î–ª—è —Ä–∞–±–æ—Ç—ã –ë–ï–ó –ò–ù–¢–ï–†–ù–ï–¢–ê –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å:")
    print(f"   1. –ü–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –æ–¥–∏–Ω —Ä–∞–∑")
    print(f"   2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º - –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–µ—Ç—Å—è –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ")
    print(f"   3. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ—Ñ–ª–∞–π–Ω")
    print(f"{'=' * 60}\n")

    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
    try:
        print(f"‚¨áÔ∏è –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å '{model_name}' –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ—Ñ—Ñ–ª–∞–π–Ω-—Ä–∞–±–æ—Ç—ã...")
        model = SentenceTransformer(model_name)
        print(f"‚û°Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É: '{local_path}'...")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(local_path, exist_ok=True)
        model.save(local_path)
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –æ—Ñ—Ñ–ª–∞–π–Ω-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.")
        return model
    except Exception as e:
        print(f"‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å: {e}")
        print("‚ùó –†–∞–±–æ—Ç–∞ –≤ –æ—Ñ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏.")
        raise


# --- –£–õ–£–ß–®–ï–ù–ù–û–ï –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í ---

def get_embeddings_with_cache(sentences: List[str], model: SentenceTransformer,
                              cache_file: str, description: str = "") -> np.ndarray:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–∞ –¥–∏—Å–∫. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞.
    """
    if os.path.exists(cache_file):
        try:
            print(f"\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({description}) –∏–∑ –∫—ç—à–∞: '{cache_file}'...")
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞
            if (isinstance(cached_data, dict) and
                    'embeddings' in cached_data and
                    'sentence_count' in cached_data and
                    cached_data['sentence_count'] == len(sentences)):

                print(f"   ‚úÖ –ö—ç—à –≤–∞–ª–∏–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(cached_data['embeddings'])} –≤–µ–∫—Ç–æ—Ä–æ–≤.")
                return cached_data['embeddings']
            else:
                print("   ‚ö†Ô∏è –ö—ç—à –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—É—â–∏–º –¥–∞–Ω–Ω—ã–º. –ü–µ—Ä–µ—Å—á–µ—Ç...")
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞: {e}. –ü–µ—Ä–µ—Å—á–µ—Ç...")

    # –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –Ω–µ–≤–∞–ª–∏–¥–µ–Ω - —Å—á–∏—Ç–∞–µ–º –∑–∞–Ω–æ–≤–æ
    print(f"\nüß† –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ({description})...")
    print("   ‚è±Ô∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å)...")

    start_time = time.time()
    embeddings = model.encode(sentences, show_progress_bar=True, batch_size=32)
    end_time = time.time()

    print(f"   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    cache_data = {
        'embeddings': embeddings,
        'sentence_count': len(sentences),
        'model_name': MODEL_NAME,
        'timestamp': time.time()
    }

    print(f"‚û°Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –∫—ç—à: '{cache_file}'...")
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f)
    print("   ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    return embeddings


# --- –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –î–£–ë–õ–ò–ö–ê–¢–û–í –° –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú –¢–ò–ü–û–í –î–ê–ù–ù–´–• ---

def find_duplicate_clusters(all_sentences: List[str], all_embeddings: np.ndarray,
                            new_sentence_indices: Set[int]) -> Dict[int, List[Dict]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DBSCAN –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {cluster_id: [–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏]}

    –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –∫–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è —Ç–µ–ø–µ—Ä—å –∏–º–µ—é—Ç —Ç–∏–ø int (–Ω–µ numpy.int64)
    """
    print(f"\nüîç –ü–æ–∏—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –ø–æ—Ä–æ–≥–æ–º —Å—Ö–æ–¥—Å—Ç–≤–∞ {THRESHOLD_SIMILARITY:.2f}...")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º DBSCAN –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    clustering = DBSCAN(
        metric='cosine',
        eps=1 - THRESHOLD_SIMILARITY,  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        min_samples=MIN_CLUSTER_SIZE
    )

    cluster_labels = clustering.fit_predict(all_embeddings)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    clusters = {}
    for idx, cluster_id in enumerate(cluster_labels):
        if cluster_id == -1:  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç —à—É–º (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
            continue

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy.int64 –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Python int
        cluster_id_int = int(cluster_id)

        if cluster_id_int not in clusters:
            clusters[cluster_id_int] = []

        sentence_info = {
            'index': int(idx),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int
            'text': all_sentences[idx],
            'is_new': idx in new_sentence_indices,
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –¥–ª—è –Ω–æ–≤—ã—Ö –∏ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            'original_index': idx if idx < len(new_sentence_indices) else idx - len(new_sentence_indices)
        }
        clusters[cluster_id_int].append(sentence_info)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –Ω–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    relevant_clusters = {}
    for cluster_id, sentences in clusters.items():
        if any(sent['is_new'] for sent in sentences):
            relevant_clusters[cluster_id] = sentences

    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(relevant_clusters)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏.")
    return relevant_clusters


# --- –ê–ù–ê–õ–ò–ó –£–ù–ò–ö–ê–õ–¨–ù–´–• –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô –° –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú –¢–ò–ü–û–í ---

def find_unique_sentences(new_embeddings: np.ndarray, base_embeddings: np.ndarray,
                          new_sentences: List[str], threshold: float = 0.6) -> List[Dict]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ—Ö–æ–∂–∏ –Ω–∏ –Ω–∞ —á—Ç–æ –≤ –±–∞–∑–µ –∏ —Å—Ä–µ–¥–∏ –Ω–æ–≤—ã—Ö.
    """
    print(f"\n‚ú® –ü–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ < {threshold:.2f})...")

    unique_sentences = []

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –±–∞–∑–æ–π
    if len(base_embeddings) > 0:
        base_similarity = cosine_similarity(new_embeddings, base_embeddings)
        max_base_similarity = np.max(base_similarity, axis=1)
    else:
        max_base_similarity = np.zeros(len(new_sentences))

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–µ–∂–¥—É —Å–æ–±–æ–π
    if len(new_embeddings) > 1:
        new_similarity = cosine_similarity(new_embeddings)
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –¥—Ä—É–≥–∏–º–∏ –Ω–æ–≤—ã–º–∏
        np.fill_diagonal(new_similarity, 0)  # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–∞–º–∏–º —Å–æ–±–æ–π
        max_new_similarity = np.max(new_similarity, axis=1)
    else:
        max_new_similarity = np.zeros(len(new_sentences))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    for i, sentence in enumerate(new_sentences):
        if (max_base_similarity[i] < threshold and
                max_new_similarity[i] < threshold):
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy float –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π float
            unique_sentences.append({
                'index': int(i),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int
                'text': sentence,
                'max_base_similarity': float(max_base_similarity[i]),
                'max_new_similarity': float(max_new_similarity[i])
            })

    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_sentences)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.")
    return unique_sentences


# --- –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ù–´–ô –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---

def print_results(clusters: Dict[int, List[Dict]], unique_sentences: List[Dict],
                  new_sentences: List[str], base_sentences: List[str]):
    """
    –ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞.
    """
    print(f"\n{'=' * 80}")
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –î–£–ë–õ–ò–ö–ê–¢–û–í")
    print(f"{'=' * 80}")

    if not clusters and not unique_sentences:
        print("‚ÑπÔ∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –Ω–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.")
        return

    # –í—ã–≤–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    if clusters:
        print(f"\n{'-' * 80}")
        print("üéØ –ù–ê–ô–î–ï–ù–´ –ö–õ–ê–°–¢–ï–†–´ –î–£–ë–õ–ò–ö–ê–¢–û–í:")
        print(f"{'-' * 80}")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ ID –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞
        for cluster_id in sorted(clusters.keys()):
            sentences = clusters[cluster_id]
            print(f"\nüìã –ö–ª–∞—Å—Ç–µ—Ä #{cluster_id + 1} ({len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)")
            print("   " + "-" * 50)

            new_in_cluster = [s for s in sentences if s['is_new']]
            base_in_cluster = [s for s in sentences if not s['is_new']]

            if new_in_cluster:
                print(f"üÜï –ù–û–í–´–ï –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ ({len(new_in_cluster)}):")
                for sent in new_in_cluster:
                    print(f"   ‚Ä¢ [A{sent['original_index']}] {sent['text']}")

            if base_in_cluster:
                print(f"\nüíæ –°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –±–∞–∑–µ ({len(base_in_cluster)}):")
                for sent in base_in_cluster:
                    original_idx = sent['original_index']
                    print(f"   ‚Ä¢ [B{original_idx}] {sent['text']}")

    # –í—ã–≤–æ–¥ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    if unique_sentences:
        print(f"\n{'-' * 80}")
        print("‚ú® –£–ù–ò–ö–ê–õ–¨–ù–´–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø (–Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –±–∞–∑–æ–π –∏ –Ω–æ–≤—ã–º–∏):")
        print(f"{'-' * 80}")

        for i, sent in enumerate(unique_sentences, 1):
            print(f"\nüíé –£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ #{i}:")
            print(f"   ‚Ä¢ [A{sent['index']}] {sent['text']}")
            print(f"   üìä –ú–∞–∫—Å. —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –±–∞–∑–æ–π: {sent['max_base_similarity']:.3f}")
            print(f"   üìä –ú–∞–∫—Å. —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –Ω–æ–≤—ã–º–∏: {sent['max_new_similarity']:.3f}")

    print(f"\n{'=' * 80}")
    print(f"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: {len(clusters)}")
    print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(unique_sentences)}")
    print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(new_sentences)}")
    print(f"   ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –±–∞–∑–µ: {len(base_sentences)}")
    print(f"{'=' * 80}")


# --- –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ü–û–î–ì–û–¢–û–í–ö–ò –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ö –°–û–•–†–ê–ù–ï–ù–ò–Æ –í JSON ---

def prepare_results_for_json(clusters: Dict[int, List[Dict]], unique_sentences: List[Dict],
                             new_sentences: List[str], base_sentences: List[str]) -> Dict:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –≤—Å–µ numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã.
    """
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é clusters —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    json_clusters = {}
    for cluster_id, sentences in clusters.items():
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–ª—é—á –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        json_cluster_id = str(cluster_id)

        json_sentences = []
        for sent in sentences:
            json_sentences.append({
                'index': int(sent['index']),
                'text': str(sent['text']),
                'is_new': bool(sent['is_new']),
                'original_index': int(sent['original_index'])
            })
        json_clusters[json_cluster_id] = json_sentences

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    json_unique = []
    for sent in unique_sentences:
        json_unique.append({
            'index': int(sent['index']),
            'text': str(sent['text']),
            'max_base_similarity': float(sent['max_base_similarity']),
            'max_new_similarity': float(sent['max_new_similarity'])
        })

    return {
        'clusters': json_clusters,
        'unique_sentences': json_unique,
        'summary': {
            'total_clusters': len(clusters),
            'total_unique': len(unique_sentences),
            'total_new_sentences': len(new_sentences),
            'total_base_sentences': len(base_sentences),
            'threshold_used': THRESHOLD_SIMILARITY
        },
        'metadata': {
            'model_used': MODEL_NAME,
            'timestamp': time.time(),
            'analysis_version': '1.1'
        }
    }


# --- –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ï–ô –ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú –û–®–ò–ë–ö–ò ---

def analyze_sentences(new_sentences: List[str], base_sentences: List[str]):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å.
    """
    print(f"üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô")
    print(f"   ‚Ä¢ –ù–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(new_sentences)}")
    print(f"   ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –±–∞–∑–µ: {len(base_sentences)}")
    print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   ‚Ä¢ –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞: {THRESHOLD_SIMILARITY:.2f}")

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ—Ñ–ª–∞–π–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)
        model = load_model_offline(MODEL_NAME, LOCAL_MODEL_PATH)

        # 2. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–±—â–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        all_sentences = new_sentences + base_sentences
        new_indices = set(range(len(new_sentences)))  # –ò–Ω–¥–µ–∫—Å—ã –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        # 3. –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        all_embeddings = get_embeddings_with_cache(
            all_sentences,
            model,
            "all_embeddings_cache.pkl",
            "–≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–æ–≤—ã–µ + –±–∞–∑–∞)"
        )

        # 4. –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        clusters = find_duplicate_clusters(all_sentences, all_embeddings, new_indices)

        # 5. –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –Ω–æ–≤—ã—Ö
        new_embeddings = all_embeddings[:len(new_sentences)]
        base_embeddings = all_embeddings[len(new_sentences):] if base_sentences else np.array([])

        unique_sentences = find_unique_sentences(
            new_embeddings,
            base_embeddings,
            new_sentences,
            threshold=THRESHOLD_SIMILARITY - 0.25  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        )

        # 6. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print_results(clusters, unique_sentences, new_sentences, base_sentences)

        # 7. –ü–û–î–ì–û–¢–ê–í–õ–ò–í–ê–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø JSON (–ò–°–ü–†–ê–í–õ–ï–ù–û)
        results_for_json = prepare_results_for_json(clusters, unique_sentences, new_sentences, base_sentences)

        # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        results_file = 'analysis_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_for_json, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{results_file}'")

        return results_for_json

    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ—à–µ–Ω–∏—é:")
        print("   ‚Ä¢ –î–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")
        print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ (–º–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç ~1.5 GB)")
        print("   ‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏")
        # –í—ã–≤–æ–¥–∏–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"   ‚Ä¢ –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
        if hasattr(e, '__traceback__'):
            import traceback
            print("\nüìù –î–µ—Ç–∞–ª–∏ —Å—Ç–µ–∫–∞ –≤—ã–∑–æ–≤–æ–≤:")
            traceback.print_exc()
        raise


# --- –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ï –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ---

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

    # 100 –ù–û–í–´–• –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (A) - –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ—Ä–µ–º –º–µ–Ω—å—à–µ
    new_sentences_demo = [
        # –ö–ª–∞—Å—Ç–µ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ 1
        "–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∑–∞—è–≤–æ–∫ –±—ã—Å—Ç—Ä–µ–µ.",
        "–£—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤.",
        "–°–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø—Ä–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.",

        # –ö–ª–∞—Å—Ç–µ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ 2
        "–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∫–Ω–æ–ø–∫—É –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.",
        "–í–Ω–µ–¥—Ä–∏—Ç—å –∏–∫–æ–Ω–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º.",

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        "–û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–π —Å–µ–º–∏–Ω–∞—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –Ω–æ–≤—ã–º –º–æ–¥—É–ª–µ–º.",
        "–í–Ω–µ–¥—Ä–∏—Ç—å –¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—É—é –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.",
        "–î–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV.",
        "–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞—É–¥–∏—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π IT-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã.",
        "–û–±–µ—Å–ø–µ—á–∏—Ç—å –æ–±–µ–¥—ã –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–∏–∫–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–π –±–µ—Å–ø–ª–∞—Ç–Ω–æ.",
        "–ì–ª–∞–≤–Ω–æ–≥–æ –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∫–æ—Ä–º–∏—Ç—å –∑–∞ —Å—á–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.",
    ]

    # 5000 –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô –ë–ê–ó–´ (B) - –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ—Ä–µ–º –º–µ–Ω—å—à–µ
    base_sentences_demo = [
        "–ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç '–ü–æ–∏—Å–∫' –≤ –≤–µ—Ä—Ö–Ω–∏–π –ª–µ–≤—ã–π —É–≥–æ–ª.",
        "–°–¥–µ–ª–∞—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º.",
        "–ò–∑–º–µ–Ω–∏—Ç—å —Ü–≤–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ —Å–∏–Ω–∏–π.",
        "–î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º —á–µ—Ä–µ–∑ –∏–∫–æ–Ω–∫—É.",
        "–ü—Ä–æ–≤–µ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –ø–æ –Ω–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.",
        "–¥–ª—è –Ω–∞—á–∞–ª—å–Ω–∏–∫–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ –≤ —Å—Ç–æ–ª–æ–≤–æ–π.",
    ]

    print("üß™ –ó–ê–ü–£–°–ö –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–û–ì–û –†–ï–ñ–ò–ú–ê")
    print("   (–í —Ä–µ–∞–ª—å–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∑–∞–º–µ–Ω–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–∞—à–∏)")

    results = analyze_sentences(new_sentences_demo, base_sentences_demo)

    print("\n‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ô –†–ï–ñ–ò–ú –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")